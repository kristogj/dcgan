import torch.nn as nn

config = {
    "batch_size": 128,  # Mini-batch size used during training
    "image_size": 64,  # Resize all input image to this size
    "nc": 3,  # Number of channels for input images (RGB)
    "nz": 100,  # Size of z latent vector drawn fro standard normal distribution by Generator
    "ngf": 64,  # Size of feature maps in generator
    "ndf": 64,  # Size of feature maps in discriminator
    "epochs": 10,
    "lr": 2e-4,  # Learning rate for Adam Optimizer
    "beta1": 0.5,  # Beta hyperparameter for Adam Optimizer
}


class Generator(nn.Module):

    def __init__(self, config):
        super(Generator, self).__init__()

        # nc: Number of channels for RGB image
        # nz: Size of z latent vector
        # ngf: Size of feature maps in generator
        nc, nz, ngf = config["nc"], config["nz"], config["ngf"]
        self.main = nn.Sequential(
            # Input is Z, going into a convolution
            nn.ConvTranspose2d(nz, 8 * ngf, kernel_size=4, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(8 * ngf),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(8 * ngf, 4 * ngf, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(4 * ngf),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(4 * ngf, 2 * ngf, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(2 * ngf),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(2 * ngf, ngf, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(ngf, nc, kernel_size=4, stride=2, padding=1, bias=False),
            nn.Tanh()
            # Output is now of size (nc) x 64 x 64
        )

    def forward(self, input):
        return self.main(input)


class Discriminator(nn.Module):

    def __init__(self, config):
        super(Discriminator, self).__init__()
        # nc: Number of channels for RGB image
        # nz: Size of z latent vector
        # ndf: Size of feature maps in discriminator
        nc, nz, ndf = config["nc"], config["nz"], config["ndf"]
        self.main = nn.Sequential(
            # Input is a nc x 64 x64 image
            nn.Conv2d(nc, ndf, kernel_size=4, stride=2, padding=1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ndf, 2 * ndf, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(2 * ndf),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(2 * ndf, 4 * ndf, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(4 * ndf),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(4 * ndf, 8 * ndf, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(8 * ndf),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(8 * ndf, 1, kernel_size=4, stride=1, padding=0, bias=False),
            nn.Sigmoid()
            # The Discriminator is a binary classifier who predicts if an image is Fake (generated by Generator) or real
        )

    def forward(self, input):
        return self.main(input)
